{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "esvUnbFXWgcr",
    "ExecuteTime": {
     "end_time": "2023-12-22T11:41:52.879500200Z",
     "start_time": "2023-12-22T11:41:52.848165600Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import field, dataclass\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    HfArgumentParser,\n",
    "    EarlyStoppingCallback,\n",
    "    IntervalStrategy,\n",
    "    AlbertTokenizer,\n",
    "    BertTokenizer,\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from transformers.utils import ModelOutput\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# global constants for this script\n",
    "SPAN_PAD = [3, 5]\n",
    "LABEL_PAD = -100\n",
    "LABEL_PAD_LIGHT = 0\n",
    "UNK = 1\n",
    "MAX_SENT_LENGTH = 500\n",
    "THRESHOLD = 2\n",
    "KEYS = ('content', 'label')\n",
    "EXCEPT_KEYS = ('spans', 'input_lengths')"
   ],
   "metadata": {
    "id": "63AyaWz-KkAr",
    "ExecuteTime": {
     "end_time": "2023-12-22T11:41:53.316355Z",
     "start_time": "2023-12-22T11:41:53.287058800Z"
    }
   },
   "execution_count": 96,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# arguments:\n",
    "# ModelArguments for model initialize; ControlArguments for writing and reading related operation\n",
    "# CustomTrainArguments inherited from Huggingface TrainArguments.\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "\n",
    "  num_layers: int = field(default=2, metadata={'help': 'number of layers for RNNs'})\n",
    "  input_dim: int = field(default=100, metadata={'help': 'number of dimension for word tokens'})\n",
    "  hidden_dim: int = field(default=768)\n",
    "  vector_dim: int = field(default=100)\n",
    "  char_hidden_dim: int = field(default=50)\n",
    "  model_path: str = field(default='cache/albert-English')\n",
    "  pretrained_tokenizer: bool = field(default=False)\n",
    "  word_vector: str = field(default='word2vector/glove.6B.100d.txt')\n",
    "\n",
    "@dataclass\n",
    "class CustomTrainArguments(TrainingArguments):\n",
    "  evaluation_strategy: str = field(default='steps')\n",
    "  output_dir: str = field(default='saved_model')\n",
    "  per_device_train_batch_size: int = field(default=16)\n",
    "  learning_rate: float = field(default=5e-5)\n",
    "  num_train_epochs: int = field(default=5)\n",
    "  eval_steps: int = field(default=5)\n",
    "  save_total_limit: int = field(default=5)\n",
    "  load_best_model_at_end: bool = field(default=True)\n",
    "  metric_for_best_model: str = field(default='accuracy')\n",
    "\n",
    "@dataclass\n",
    "class ControlArguments:\n",
    "  log_dir: str = field(default='log/', metadata={'help': 'directory to save log files'})\n",
    "  dataset_path: str = field(default='spam-test/')\n",
    "  reader: str = field(default='csv', metadata={'help': 'methods used to read source file'})\n",
    "  task_type: str = field(default='sent')\n",
    "  save_to: str = field(default='saved_model', metadata={'help': 'directory used to save model'})\n",
    "  label_mapping: str = field(default='label_mapping.json', metadata={'help': 'saved label mapping for some specific tasks with string label'})\n",
    "  best_model_path: str = field(default='saved_model/roberta-chinese_5.pth')\n",
    "  given_best_model: bool = field(default=False, metadata={'help': 'if give best model path'})\n",
    "  is_light: bool = field(default=True, metadata={'help': 'whether train in light mode '})\n",
    "  cached_tokenizer: str = field(default='cache/cache_tokenizer.bin')\n",
    "  seed: int = field(default=42)\n",
    "\n",
    "\n",
    "\n",
    "control_arguments, train_arguments, model_arguments = ControlArguments(), CustomTrainArguments(), ModelArguments()\n",
    "\n",
    "# init_args()\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "tPDRosvNXMEy",
    "outputId": "7b536712-e4b6-4c85-c0b2-7fbc8be472cd",
    "ExecuteTime": {
     "end_time": "2023-12-21T12:05:51.346217100Z",
     "start_time": "2023-12-21T12:05:51.281427200Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# initialize Dataset class and related methods\n",
    "def csv_reader(file_path, keys=KEYS):\n",
    "    \"\"\"\n",
    "    generate data as described in keys\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    if len(keys) > 2:\n",
    "        data[keys[1]] = data[keys[1]].apply(eval)\n",
    "        data[keys[2]] = data[keys[2]].apply(eval)\n",
    "    data_ = []\n",
    "    for key in keys:\n",
    "        if key in data.columns:\n",
    "            data_.append(list(data[key]))\n",
    "        else:\n",
    "            raise KeyError(f\"{key} doesn't exist in source csv\")\n",
    "    return data_\n",
    "\n",
    "\n",
    "class SentDataset(Dataset):\n",
    "    def __init__(self, dataset_path, reader, label2idx=None):\n",
    "        self.all_samples, self.labels = reader(dataset_path)\n",
    "        if label2idx is not None:\n",
    "            self.label2idx = label2idx\n",
    "        else:\n",
    "            self.label2idx = {}\n",
    "            self._gen_label2idx()\n",
    "\n",
    "    def _gen_label2idx(self):\n",
    "        for label in self.labels:\n",
    "            if isinstance(label, Iterable) and not isinstance(label, str):\n",
    "                for inner_label in label:\n",
    "                    if inner_label not in self.label2idx:\n",
    "                        self.label2idx[inner_label] = len(self.label2idx)\n",
    "            else:\n",
    "                if label not in self.label2idx:\n",
    "                    self.label2idx[label] = len(self.label2idx)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        label_cur = self.labels[item]\n",
    "        if isinstance(label_cur, Iterable) and not isinstance(label_cur, str):\n",
    "            label_idx_cur = [self.label2idx.get(i, None) for i in label_cur]\n",
    "            for i in label_idx_cur:\n",
    "                if i is None:\n",
    "                    raise KeyError(f\"found unexisted key in {label_cur}\")\n",
    "        else:\n",
    "            label_idx_cur = self.label2idx.get(label_cur, None)\n",
    "            if label_idx_cur is None:\n",
    "                raise KeyError(f\"{label_cur} doesn't exist in label list\")\n",
    "        return self.all_samples[item], label_idx_cur\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_samples)\n",
    "\n",
    "\n",
    "class DocDataset(Dataset):\n",
    "    def __init__(self, dataset_path, reader):\n",
    "        self.all_samples, self.labels, self.entity_spans = reader(dataset_path)\n",
    "        self.label2idx = {}\n",
    "        for label in self.labels:\n",
    "            if isinstance(label, Iterable) and not isinstance(label, str):\n",
    "                for inner_label in label:\n",
    "                    if inner_label not in self.label2idx:\n",
    "                        self.label2idx[inner_label] = len(self.label2idx)\n",
    "            else:\n",
    "                if label not in self.label2idx:\n",
    "                    self.label2idx[label] = len(self.label2idx)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        label_cur = self.labels[item]\n",
    "        if isinstance(label_cur, Iterable) and isinstance(label_cur, str):\n",
    "            label_idx_cur = [self.label2idx.get(i, None) for i in label_cur]\n",
    "            for i in label_idx_cur:\n",
    "                if i is None:\n",
    "                    raise KeyError(f\"found unexisted key in {label_cur}\")\n",
    "        else:\n",
    "            label_idx_cur = self.label2idx.get(label_cur, None)\n",
    "            if label_idx_cur is None:\n",
    "                raise KeyError(f\"{label_cur} doesn't exist in label list\")\n",
    "        return self.all_samples[item], label_idx_cur, self.entity_spans[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_samples)\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "ia5gBemv8RXm",
    "outputId": "43b72087-df36-462d-afec-0d96127b5fb6",
    "ExecuteTime": {
     "end_time": "2023-12-22T11:41:58.178689400Z",
     "start_time": "2023-12-22T11:41:58.161417600Z"
    }
   },
   "execution_count": 97,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# model architecture.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BigBirdModel\n",
    ")\n",
    "\n",
    "\n",
    "class PretrainSentModel(nn.Module):\n",
    "    def __init__(self, model_path, hidden_dim, num_labels):\n",
    "        super(PretrainSentModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoder = AutoModel.from_pretrained(model_path)\n",
    "        # self.encoder = BigBirdModel.from_pretrained(model_path)\n",
    "        self.linear_layer = nn.Linear(hidden_dim, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                labels=None,\n",
    "                spans=None,\n",
    "                output_attentions=None,\n",
    "                return_dict=None\n",
    "                ):\n",
    "        # (loss[optional], logit, hidden_states[optional], output_attentions[optional]\n",
    "        output = self.encoder(input_ids=input_ids,\n",
    "                              attention_mask=attention_mask,\n",
    "                              token_type_ids=token_type_ids,\n",
    "                              position_ids=position_ids,\n",
    "                              return_dict=return_dict)\n",
    "        sequence_output = output[0]  # (batch_size, seq_len, hidden_dim)\n",
    "        last_hidden_state = sequence_output[:, -1, :]  # (batch_size, hidden_dim)\n",
    "        # batch_size, num_span = sequence_output.size(0), len(spans[0])\n",
    "\n",
    "            \n",
    "\n",
    "        pred_ = self.linear_layer(last_hidden_state)  # (batch_size, num_categories)\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(pred_, labels)\n",
    "        else:\n",
    "            loss = None\n",
    "        return ModelOutput(loss=loss, logits=pred_)\n",
    "\n",
    "    def dynamic_quantization(self):\n",
    "        quantized_model = torch.quantization.quantize_dynamic(self.encoder, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "        setattr(self, 'encoder', quantized_model)\n",
    "\n",
    "\n",
    "class LongFormer(nn.Module):\n",
    "    def __init__(self, model_path, hidden_dim, num_labels):\n",
    "        super(LongFormer, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoder = BigBirdModel.from_pretrained(model_path)\n",
    "        self.linear_layer = nn.Linear(hidden_dim, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                labels=None,\n",
    "                spans=None,\n",
    "                output_attentions=None,\n",
    "                return_dict=None\n",
    "                ):\n",
    "        # (loss[optional], logit, hidden_states[optional], output_attentions[optional]\n",
    "        output = self.encoder(input_ids=input_ids,\n",
    "                              attention_mask=attention_mask,\n",
    "                              token_type_ids=token_type_ids,\n",
    "                              position_ids=position_ids,\n",
    "                              return_dict=return_dict)\n",
    "        sequence_output = output[0]\n",
    "        batch_size, num_span = sequence_output.size(0), len(spans[0])\n",
    "        entity_embedding = torch.rand(batch_size, num_span, self.hidden_dim)\n",
    "        for idx, span_items in enumerate(spans):\n",
    "            for idx_span, span_item in enumerate(span_items):\n",
    "                entity_rep = sequence_output[idx, span_item[0]:span_item[1]]\n",
    "                entity_embedding[idx, idx_span, :] = torch.mean(entity_rep, dim=0)\n",
    "        # (batch_size, num_spans, hidden_dim) -> (batch_size, num_spans, num_categories)\n",
    "        pred_ = self.linear_layer(entity_embedding)\n",
    "        pred_ = pred_.view(batch_size * num_span, -1)  # (batch_size, number of categories)\n",
    "        return {'logits': pred_}\n"
   ],
   "metadata": {
    "id": "uIxPXyUEOHld",
    "ExecuteTime": {
     "end_time": "2023-12-22T10:36:24.847909800Z",
     "start_time": "2023-12-22T10:36:24.814939800Z"
    }
   },
   "execution_count": 76,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "DATACLASS = {\n",
    "    'sent': SentDataset,\n",
    "    'doc-span': DocDataset\n",
    "}\n",
    "\n",
    "READER = {\n",
    "    'csv': csv_reader\n",
    "}\n",
    "\n",
    "MODELS = {\n",
    "    'sent': PretrainSentModel,\n",
    "    'doc-span': LongFormer\n",
    "}\n"
   ],
   "metadata": {
    "id": "wyi2bW0Y8EDQ",
    "ExecuteTime": {
     "end_time": "2023-12-22T11:42:02.894470400Z",
     "start_time": "2023-12-22T11:42:02.883220700Z"
    }
   },
   "execution_count": 98,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "class PreTraining:\n",
    "    def __init__(self, train_arguments, model_arguments, control_arguments):\n",
    "        # model config\n",
    "        self.hidden_dim = model_arguments.hidden_dim\n",
    "        self.batch_size = train_arguments.per_device_train_batch_size\n",
    "        self.model_path = model_arguments.model_path\n",
    "        # self.tokenizer = AlbertTokenizer.from_pretrained(self.model_path)\n",
    "        self.save_to = control_arguments.save_to\n",
    "\n",
    "        # config for setting pipeline\n",
    "        self.dataset_path = control_arguments.dataset_path\n",
    "        self.reader = READER[control_arguments.reader]\n",
    "        self.Dataset = DATACLASS[control_arguments.task_type]\n",
    "        self.task_type = control_arguments.task_type\n",
    "\n",
    "        # training arguments\n",
    "        self.train_arguments = train_arguments\n",
    "\n",
    "        # post-processing after initialization\n",
    "        self.train, self.val, self.test = self.init_dataset()\n",
    "        if os.path.exists(control_arguments.label_mapping):\n",
    "            self.label2idx = json.load(open(control_arguments.label_mapping, 'r', encoding='utf-8'))\n",
    "            self.train.label2idx = self.label2idx\n",
    "        else:\n",
    "            self.label2idx = self.train.label2idx\n",
    "        #     json.dump(self.label2idx, open('label_mapping.json', 'w', encoding='utf-8'))\n",
    "        self.val.label2idx = self.label2idx\n",
    "        self.test.label2idx = self.label2idx\n",
    "\n",
    "        self.idx2label = {}\n",
    "        for key, value in self.label2idx.items():\n",
    "            self.idx2label[value] = key\n",
    "\n",
    "    def create_loader(self, collate_fn=None, data_sampler=None):\n",
    "        train_loader = DataLoader(self.train, batch_size=self.batch_size, collate_fn=collate_fn,\n",
    "                                  sampler=data_sampler, shuffle=True)\n",
    "        val_loader = DataLoader(self.val, batch_size=self.batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "        test_loader = DataLoader(self.test, batch_size=self.batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def init_dataset(self):\n",
    "        train_dataset = self.Dataset(os.path.join(self.dataset_path, 'val.csv'), self.reader)\n",
    "        val_dataset = self.Dataset(os.path.join(self.dataset_path, 'val.csv'), self.reader)\n",
    "        test_dataset = self.Dataset(os.path.join(self.dataset_path, 'val.csv'), self.reader)\n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "    def prepare_model(self):\n",
    "        if 'albert' in self.model_path or 'Albert' in self.model_path:\n",
    "            tokenizer = AlbertTokenizer.from_pretrained(self.model_path)\n",
    "        else:\n",
    "            tokenizer = BertTokenizer.from_pretrained(self.model_path)\n",
    "        try:\n",
    "            model_class = MODELS[self.task_type]\n",
    "            model = model_class(self.model_path, self.hidden_dim, len(self.label2idx))\n",
    "        except KeyError:\n",
    "            raise f\"{self.task_type} doesn't have a corresponding model \"\n",
    "        return tokenizer, model\n",
    "\n",
    "    def prepare_optimizer(self, model, data_loader):\n",
    "        all_steps = self.epoch * len(data_loader)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=self.lr, eps=self.adam_eps)\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=self.lr)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.warmup_step, num_training_steps=all_steps)\n",
    "        loss_fn = nn.CrossEntropyLoss(ignore_index=LABEL_PAD)\n",
    "        return optimizer, scheduler, loss_fn\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "YEl-FzPrbUtg",
    "ExecuteTime": {
     "end_time": "2023-12-22T11:42:03.367913300Z",
     "start_time": "2023-12-22T11:42:03.324056700Z"
    }
   },
   "execution_count": 99,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Custom hugging face trainner class\n",
    "from typing import Optional, List, Dict\n",
    "from transformers import Trainer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "  def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    labels = inputs.get('labels')\n",
    "    logit = model(**inputs).get('logits')\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(logit.view(-1, self.model.num_labels), labels.view(-1))\n",
    "    return (loss, logit) if return_outputs else loss\n",
    "  \n",
    "  def evaluate(\n",
    "        self,\n",
    "        eval_dataset: Optional[Dataset] = None,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "        metric_key_prefix: str = \"eval\",\n",
    "    ) -> Dict[str, float]:\n",
    "      self._memory_tracker.start()\n",
    "      all_pred, all_target = [], []\n",
    "      report = {}\n",
    "      correct = 0\n",
    "      local_rank = self.args.local_rank\n",
    "      dataloader = self.get_eval_dataloader(eval_dataset=eval_dataset)\n",
    "      for batch_data in dataloader:\n",
    "          target = batch_data['labels']\n",
    "          output = self.model(**batch_data)\n",
    "          pred_score = output.get('logits')\n",
    "          pred_class = torch.argmax(pred_score, dim=-1)\n",
    "          correct += torch.sum(pred_class==target).item()\n",
    "          assert target.size(0) == pred_class.size(0)\n",
    "          all_pred.append(pred_class)\n",
    "          all_target.append(target)\n",
    "      all_pred, all_target = torch.cat(all_pred, dim=0), torch.cat(all_target, dim=0)\n",
    "      if all_pred.is_cuda:\n",
    "          all_pred, all_target = all_pred.cpu().numpy(), all_target.cpu().numpy()\n",
    "      else:\n",
    "          all_pred, all_target = all_pred.numpy(), all_target.numpy()\n",
    "      all_num = all_pred.shape[0]\n",
    "    # pdb.set_trace()\n",
    "      indies = all_pred == all_target\n",
    "      correct_array = all_pred[indies]\n",
    "      correct_num = np.sum(indies)\n",
    "      acc = correct_num / all_num\n",
    "      report['accuracy'] = acc\n",
    "      print(f'number of correct: {correct}')\n",
    "      print(f'number of correct: {correct_num}; number of all samples: {all_num} ')\n",
    "          \n",
    "      return report\n",
    "  \n",
    "\n",
    "      \n",
    "def compute_metric(eval):\n",
    "  logit, label = eval\n",
    "  correct = 0\n",
    "  pred_class = torch.argmax(logit, dim=-1)\n",
    "  assert logit.size(0) == label.size(0)\n",
    "  correct += torch.sum(pred_class == label).item()\n",
    "  if logit.is_cuda:\n",
    "    pred_class, label = pred_class.cpu().numpy(), label.cpu().numpy()\n",
    "  else:\n",
    "    pred_class, label = pred_class.numpy(), label.numpy()\n",
    "  acc = accuracy_score(label, pred_class)\n",
    "  report = classification_report(label, pred_class)\n",
    "  report['accuracy'] = acc\n",
    "  return report"
   ],
   "metadata": {
    "id": "aF9gG-KQTECr",
    "ExecuteTime": {
     "end_time": "2023-12-22T12:26:09.903085100Z",
     "start_time": "2023-12-22T12:26:09.874085300Z"
    }
   },
   "execution_count": 122,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from utilis.constants import SPAN_PAD, LABEL_PAD, MAX_SENT_LENGTH\n",
    "\n",
    "\n",
    "class CollateFn:\n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 label2idx,\n",
    "                 idx2label=None,\n",
    "                 is_split=False,\n",
    "                 task_type='doc-span'\n",
    "                 ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2idx = label2idx\n",
    "        if idx2label:\n",
    "            self.idx2label = idx2label\n",
    "        else:\n",
    "            self.idx2label = {}\n",
    "            for label, idx in label2idx.items():\n",
    "                self.idx2label[idx] = label\n",
    "        self.is_split = is_split\n",
    "        self.task_type = task_type\n",
    "\n",
    "    def __call__(self, batch_data):\n",
    "        batchfy_input, batch_data_sep = self.processing(batch_data)\n",
    "        batchfy_input = self.post_process(batchfy_input, batch_data_sep)\n",
    "        return batchfy_input\n",
    "\n",
    "    def processing(self, batch_data):\n",
    "        # (all_text, all_label, entity_spans[optional])\n",
    "        batch_data_sep = _pre_processing(batch_data, task_type=self.task_type)\n",
    "        batchfy_input = self.tokenizer(batch_data_sep[0],\n",
    "                                       is_split_into_words=self.is_split,\n",
    "                                       truncation=True,\n",
    "                                       padding=True,\n",
    "                                       return_tensors='pt',\n",
    "                                       max_length=MAX_SENT_LENGTH,\n",
    "                                       )\n",
    "        return batchfy_input, batch_data_sep\n",
    "\n",
    "    def post_process(self, batchfy_input, batch_data_sep):\n",
    "        if self.task_type == 'doc-span':\n",
    "\n",
    "            # pad_labels, pad_spans = _padding_token(all_labels, all_spans)\n",
    "            pad_labels, pad_spans = _padding_entity(batchfy_input[1], batchfy_input[2])\n",
    "            # pad_spans_df = _span_to_csv(pad_spans)\n",
    "            batchfy_input['labels'] = torch.tensor(pad_labels, dtype=torch.long)\n",
    "            batchfy_input['spans'] = pad_spans\n",
    "            return batchfy_input\n",
    "        else:\n",
    "            batchfy_input['labels'] = torch.tensor(batch_data_sep[1], dtype=torch.long)\n",
    "\n",
    "        return batchfy_input\n",
    "\n",
    "def _pre_processing(batch_data, task_type):\n",
    "    \"\"\"\n",
    "    processing batch data from dataloader before feeding into model depends on task type\n",
    "    return:\n",
    "    all_text: list[str]\n",
    "    all_label: list[int] (see this task as a token classification tasks)\n",
    "    entity_spans: pd.Dataframe (in this way, model can select list of indexes effciently\n",
    "    \"\"\"\n",
    "    all_text, all_label = [], []\n",
    "    if task_type == 'doc-span':\n",
    "        entity_spans = []\n",
    "    else:\n",
    "        entity_spans = None\n",
    "    for zip_sample in batch_data:\n",
    "        text, label = zip_sample[0], zip_sample[1]\n",
    "        all_text.append(text)\n",
    "        all_label.append(label)\n",
    "        if entity_spans is not None:\n",
    "            entity_spans.append(zip_sample[2])\n",
    "    # spans_csv = pd.DataFrame(entity_spans, columns=[for ])\n",
    "    return (all_text, all_label, entity_spans) if entity_spans else (all_text, all_label)\n",
    "\n",
    "\n",
    "def _padding_token(labels, spans):\n",
    "    \"\"\"\n",
    "    padding labels into same length of spans and covert label format to token classification format\n",
    "    \"\"\"\n",
    "    max_len = max(map(len, labels))\n",
    "    pad_spans = []\n",
    "    label_in_token = []\n",
    "    for label_item, span_item in zip(labels, spans):\n",
    "        cur_label_token = []\n",
    "        for label_inner, span_inner in zip(label_item, span_item):\n",
    "            cur_label_token += [label_inner] * (span_inner[1] - span_inner[0])\n",
    "        label_in_token.append(cur_label_token)\n",
    "        pad_num = (max_len - len(label_item))\n",
    "        pad_spans.append(span_item + [SPAN_PAD] * pad_num)\n",
    "    max_len_label = max(map(len, label_in_token))\n",
    "    pad_label_token = []\n",
    "    for label_token_item in label_in_token:\n",
    "        pad_num = max_len_label - len(label_token_item)\n",
    "        pad_label_token.append(label_token_item + [LABEL_PAD] * pad_num)\n",
    "    return pad_label_token, pad_spans\n",
    "\n",
    "\n",
    "def _padding_entity(labels, spans):\n",
    "    \"\"\"\n",
    "    padding labels and span's entity in a classification format, using average\n",
    "    entity representation to do prediction\n",
    "    \"\"\"\n",
    "    max_len = max(map(len, labels))\n",
    "    pad_labels, pad_spans = [], []\n",
    "    for label_item, span_item in zip(labels, spans):\n",
    "        pad_num = max_len - len(label_item)\n",
    "        pad_labels.append(label_item + [LABEL_PAD] * pad_num)\n",
    "        pad_spans.append(span_item + [SPAN_PAD] * pad_num)\n",
    "    return pad_labels, pad_spans\n",
    "\n"
   ],
   "metadata": {
    "id": "LZr9JY5dlxrm",
    "ExecuteTime": {
     "end_time": "2023-12-22T12:26:10.561137100Z",
     "start_time": "2023-12-22T12:26:10.503132700Z"
    }
   },
   "execution_count": 123,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def fix_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def train(train_arguments, model_arguments, control_arguments):\n",
    "    fix_seed(control_arguments.seed)\n",
    "\n",
    " \n",
    "    prepare = PreTraining(train_arguments, model_arguments, control_arguments)\n",
    "\n",
    "    setattr(train_arguments, 'label_names', list(prepare.idx2label.keys()))\n",
    "    # device = torch.device(control_arguments.device)\n",
    "    data_sampler = None\n",
    "    tokenizer, model = prepare.prepare_model()\n",
    "    \n",
    "    collate_fn = CollateFn(tokenizer, prepare.label2idx, task_type=prepare.task_type)\n",
    "    # train_loader, val_loader, test_loader = prepare.create_loader(collate_fn=collate_fn, data_sampler=data_sampler)\n",
    "    # model.to(device=device)\n",
    "    # optimizer, scheduler, loss_fn = prepare.prepare_optimizer(model, train_loader)\n",
    "    def preprocess_logits_for_metrics(logit, labels):\n",
    "        if logit is tuple:\n",
    "            return logit[0]\n",
    "        else:\n",
    "            return logit   \n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=train_arguments,\n",
    "            train_dataset=prepare.train,\n",
    "            eval_dataset=prepare.train,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=collate_fn,\n",
    "            compute_metrics=compute_metric,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "            preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "            \n",
    "        )\n",
    "    return trainer\n"
   ],
   "metadata": {
    "id": "kTZK1Y_fh6vm",
    "ExecuteTime": {
     "end_time": "2023-12-22T12:26:11.058187100Z",
     "start_time": "2023-12-22T12:26:11.004183900Z"
    }
   },
   "execution_count": 124,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "trainer = train(train_arguments, model_arguments, control_arguments)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T12:26:13.730320800Z",
     "start_time": "2023-12-22T12:26:11.421385300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/20 : < :, Epoch 0.25/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of correct: 20\n",
      "number of correct: 20; number of all samples: 50 \n",
      "number of correct: 20\n",
      "number of correct: 20; number of all samples: 50 \n",
      "number of correct: 20\n",
      "number of correct: 20; number of all samples: 50 \n",
      "number of correct: 20\n",
      "number of correct: 20; number of all samples: 50 \n",
      "number of correct: 20\n",
      "number of correct: 20; number of all samples: 50 \n"
     ]
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=20, training_loss=2.132588005065918, metrics={'train_runtime': 217.297, 'train_samples_per_second': 1.15, 'train_steps_per_second': 0.092, 'total_flos': 0.0, 'train_loss': 2.132588005065918, 'epoch': 5.0})"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T12:30:12.521053200Z",
     "start_time": "2023-12-22T12:26:34.934995400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
